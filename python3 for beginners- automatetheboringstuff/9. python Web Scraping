## Web Scraping:
----------------

-> Web scraping is the term for using a program to download and process content from the Web.
   Eg: Google runs many web scraping programs to index web pages for its search engine.

 Modules:
 --------
 -> webbrowser: Comes with Python and opens a browser to a specific page.
 
 -> Requests: Downloads files and web pages from the Internet.
 
 -> Beautiful Soup: Parses HTML, the format that web pages are written in.
 
 -> Selenium: Launches and controls a web browser. Selenium is able to fill in forms and simulate mouse clicks in this browser


-> The webbrowser module’s open() function can launch a new browser to a specified URL. 
Eg:
>>> import webbrowser
>>> webbrowser.open('http://inventwithpython.com/')


# Downloading Files from the Web with the requests Module:
----------------------------------------------------------
-> The requests module lets you easily download files from the Web without having to worry about complicated issues such as 
   network errors, connection problems, and data compression.
-> install it : $pip install requests


 # Downloading a Web Page with the requests.get() Function:
 ----------------------------------------------------------
 -> requests.get(): function takes a string of a URL to download, it returns a Response object.

 Eg:
 >>> import requests
 >>> res = requests.get('https://automatetheboringstuff.com/files/rj.txt')
 >>> type(res)
 <class 'requests.models.Response'>
 >>> res.status_code == requests.codes.ok
 True
 >>> len(res.text)
 178981
 >>> print(res.text[:250])
 The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare


 # Checking for Errors:
 ----------------------
 -> A simpler way to check for success is to call the raise_for_status() method on the Response object. This will raise an 
 	exception if there was an error downloading the file and will do nothing if the download succeeded.

 	Eg:
 	>>> res = requests.get('http://inventwithpython.com/page_that_does_not_exist')
	>>> res.raise_for_status()
	Traceback (most recent call last):
	  File "<pyshell#138>", line 1, in <module>
	    res.raise_for_status()
	  File "C:\Python34\lib\site-packages\requests\models.py", line 773, in raise_for_status
	    raise HTTPError(http_error_msg, response=self)
	requests.exceptions.HTTPError: 404 Client Error: Not Found


 # Saving Downloaded Files to the Hard Drive:
 --------------------------------------------
 -> Open a file in write binary mode (wb) to maintain the Unicode encoding of the text.
 -> To write the web page to a file, you can use a for loop with the Response object’s iter_content() method.
 -> iter_content(): returns chunks of content on each iteration (chunk size(in bytes) passed as parameter)

 	Eg:
 	>>> import requests
	>>> res = requests.get('https://automatetheboringstuff.com/files/rj.txt')
	>>> res.raise_for_status()
	>>> playFile = open('RomeoAndJuliet.txt', 'wb')
	>>> for chunk in res.iter_content(100000):
	        playFile.write(chunk)
	
	100000
	78981
	>>> playFile.close()
	
    Note: We can download and save the image to the hard drive with iter_content().

# Parsing HTML with the BeautifulSoup Module:
---------------------------------------------
-> installation: $pip install beautifulsoup4
-> import: import bs4


 # Creating a BeautifulSoup Object from HTML:
 --------------------------------------------
 -> bs4.BeautifulSoup(): takes html as string and returns a BeautifulSoup object.

 	Eg:
 	>>> import requests, bs4
	>>> res = requests.get('http://nostarch.com')
	>>> res.raise_for_status()
	>>> noStarchSoup = bs4.BeautifulSoup(res.text)
	>>> type(noStarchSoup)
	<class 'bs4.BeautifulSoup'>

 -> Once you have a BeautifulSoup object, you can use its methods to locate specific parts of an HTML document.


 # Finding an Element with the select() Method:
 ----------------------------------------------
 -> You can retrieve a web page element from a BeautifulSoup object by calling the select()method and passing a string of a 
 	CSS selector for the element you are looking for.

 	Eg:
 	soup.select('div')
 	soup.select('#author')
 	soup.select('.notice'), etc.

 -> select() method will return a list of Tag objects.
 -> Tag values can be passed to the str() function to show the HTML tags they represent.
 -> Tag values also have an attrs attribute that shows all the HTML attributes of the tag as a dictionary. 
 
    Eg:
    >>> import bs4
    >>> exampleFile = open('example.html')
    >>> exampleSoup = bs4.BeautifulSoup(exampleFile.read())
    >>> elems = exampleSoup.select('#author')
    >>> type(elems)
    <class 'list'>
    >>> len(elems)
    1
    >>> type(elems[0])
    <class 'bs4.element.Tag'>
    >>> elems[0].getText()
    'Al Sweigart'
    >>> str(elems[0])
    '<span id="author">Al Sweigart</span>'
    >>> elems[0].attrs
    {'id': 'author'}


 # Getting Data from an Element’s Attributes:
 --------------------------------------------
 -> We use get() method for Tag objects. pass attribute name and it will return attribute value.
 
    Eg:
    >>> str(spanElem)
    '<span id="author">Al Sweigart</span>'
    >>> spanElem.get('id')
    'author'
    
-> For the websites where we need to login first, we will use selenium module.


# Controlling the Browser with the selenium Module:
---------------------------------------------------
-> The selenium module lets Python directly control the browser by programmatically clicking links and filling in 
   login information, almost as though there is a human user interacting with the page.
   
   
 # Starting a Selenium-Controlled Browser:
 -----------------------------------------
 -> senenium will launch firefox bydefault.
 
    Eg:
    >>> from selenium import webdriver
    >>> browser = webdriver.Firefox()    ---> Firefox web browser starts up
    >>> type(browser)
    <class 'selenium.webdriver.firefox.webdriver.WebDriver'>
    >>> browser.get('http://inventwithpython.com')    ---> directs the browser to http://inventwithpython.com/


 # Finding Elements on the Page:
 -------------------------------
 -> 

















